---
title: "Data Mining in Twitter - Proof of Concept"
author: "Kevin A. Ryan (JHUAPL)"
date: "Monday, September 28, 2015"
output:
  word_document: default
  html_document:
    highlight: kate
    theme: cerulean
---

Setup code to enable access to Twitter API.  Explain the process here.  WHat is involved.


```{r eval=FALSE, warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
        consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
        consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
        access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
        access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

#tweets <- userTimeline("realDonaldTrump", n = 3200)
HoCo<-searchTwitter("", geocode='8.908132, 39.337193,100mi', n = 1500)


tweets<-searchTwitter("#syria", geocode='36.2301786,37.2280928, 100mi', n =3500)
tweets2<- twListToDF(tweets)
#tweets<-searchTwitter("#sudan", geocode='15.516763, 32.141242, 5000mi', n =35)

#al shabab:  @HSMPress
#Tripoli: 32.882108, 13.190482
#libya: 26.3339896,17.26921
#Mauritania: 19.630928, -10.040291
#Ethiopa: 9.485258, 39.928106
#columbia, md: 39.214928, -76.855298
## Search between two dates
        #searchTwitter('charlie sheen', since='2011-03-01', until='2011-03-02')
## geocoded results
        #searchTwitter('patriots', geocode='42.375,-71.1061111,10mi')
## using resultType
        #searchTwitter('world cup+brazil', resultType="popular", n=15)
        #searchTwitter('from:hadleywickham', resultType="recent", n=10)
        #searchTwitter("#beer", n=100)
```


----
```{r eval=FALSE}
library(twitteR) ## retrieve tweets from Twitter
tweets2<- twListToDF(tweets)# convert tweets to a data frame
write.csv(tweets2, file = "tweets2")#shortcut for saving tweets to local maching

```

Build a corpus and clean up 
```{r warning=FALSE}
library(tm)
library(NLP)

CAR2 <- read.csv("~/_data/TwitterAnalysis/CAR1")#load the file saved locally
myCorpus <- Corpus(VectorSource(CAR2$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myStopwords <- c(stopwords('english'), "available", "via", "amp") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```

Stem Completion step
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```

Frequent Words and Associations
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])

library(ggplot2)

ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()

# which words are associated with the term "tax" ?
(findAssocs(tdm, "isi", 0.2))
```

WOrd cloud mutherfucker!
```{r warnings=FALSE}
# plot word cloud
library(wordcloud)
par(mar=c(.5,.5,.5,.5))
 m <- as.matrix(tdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq <- subset(wordFreq, wordFreq >= 5)
wordFreq <- subset(wordFreq, wordFreq <= 115)
df2 <- data.frame(term = names(wordFreq), freq = wordFreq)

# word cloud
set.seed(375) # to make it reproducible

wordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)
```


```{r}
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=5)# cut tree into XX clusters
```

topic modeling2
```{r}
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
clinton2 <- clinton[1:2500,] 
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 20))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topics <- data.frame(date=as.Date(libya$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")

qplot(date, ..count.., data=topics, geom="bar",fill=term[topic], position="stack")

ggplot(topics, aes(x = date, y=..count..)) + geom_point() + facet_grid(~ topic)

ggplot(topics, aes(x = date, y=..count..)) + geom_bar() + facet_grid(~ topic)
```


