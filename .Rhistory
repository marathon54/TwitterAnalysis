plot(fit)
rect.hclust(fit, k = 6) # cut tree into 6 clusters
plot(fit)
rect.hclust(fit, k =4) # cut tree into 6 clusters
distMatrix <- dist(scale(df2))
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# calculate the frequency of words and sort it descendingly by frequency
m2 <- sort(rowSums(m2), decreasing=TRUE)
m2 <- subset(wordFreq, wordFreq >= 5)
m2 <- subset(wordFreq, wordFreq <= 115)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")
plot(fit)
rect.hclust(fit, k =4) # cut tree into 6 clusters
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# calculate the frequency of words and sort it descendingly by frequency
m2 <- sort(rowSums(m2), decreasing=TRUE)
m2 <- subset(wordFreq, wordFreq >= 35)
m2 <- subset(wordFreq, wordFreq <= 115)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")
plot(fit)
rect.hclust(fit, k =4) # cut tree into 6 clusters
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# calculate the frequency of words and sort it descendingly by frequency
m2 <- sort(rowSums(m2), decreasing=TRUE)
m2 <- subset(wordFreq, wordFreq >= 35)
m2 <- subset(wordFreq, wordFreq <= 115)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")
plot(fit)
rect.hclust(fit, k =6) # cut tree into 6 clusters
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward.D2")
plot(fit)
rect.hclust(fit, k =6) # cut tree into 6 clusters
distMatrix <- dist(scale(m2))
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward")
plot(fit)
rect.hclust(fit, k = 6) # cut tree into 6 clusters
dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
(term <- terms(lda, 6)) # first 6 terms of every topic
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
library(tm)
dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
(term <- terms(lda, 6)) # first 6 terms of every topic
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
library(topicmodels)
dtm <- as.DocumentTermMatrix(tdm)
lda <- LDA(dtm, k = 8) # find 8 topics
dtm
# transpose the matrix to cluster documents (tweets)
m3 <- t(m2)
# set a fixed random seed
set.seed(122)
# k-means clustering of tweets
k <- 8
kmeansResult <- kmeans(m3, k)
# cluster centers
round(kmeansResult$centers, digits=3)
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep=""))
s <- sort(kmeansResult$centers[i,], decreasing=T)
cat(names(s)[1:3], "\n")
# print the tweets of every cluster
# print(rdmTweets[which(kmeansResult$cluster==i)])
}
dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
rowTotals <- apply(dtm , 1, sum)
dtm <- as.DocumentTermMatrix(tdm)
rowTotals <- apply(dtm , 1, sum)
dtm.new   <- dtm[rowTotals> 0, ]
library(topicmodels)
lda <- LDA(dtm.new, k = 8) # find 8 topics
(term <- terms(lda, 6)) # first 6 terms of every topic
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(tweets.df$created), topic)
library(tm)
library(topicmodels)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
clinton2 <- clinton[1:1437,]
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 6))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
#SECOND TRY
topics <- data.frame(date=as.Date(clinton2$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
```
```{r eval=FALSE, warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
``{r eval=FALSE}
## retrieve tweets from Twitter
library(twitteR)
Clinton <-userTimeline('HillaryClinton' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
# convert tweets to a data frame
tweets.hc <- twListToDF(Clinton)
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.$text[i], 60))
}
#shortcut for saving tweets to local maching
write.csv(tweets.hc, file = "clinton")
library(twitteR)
Clinton <-userTimeline('HillaryClinton' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
tweets.hc <- twListToDF(Clinton)
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.$text[i], 60))
}
tweets.hc <- twListToDF(Clinton)
tweets.hc
for (i in c(1:2, 1500)) {
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.$text[i], 60))
}
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.hc$text[i], 60))
}
dim(tweets.hc)
write.csv(tweets.hc, file = "clinton")
#load the file saved locally
clinton <- read.csv("~/_data/TwitterAnalysis/clinton")
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(clinton$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
#remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
#need to explain the concept of stop words.....
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "will", "hillari", "rt", "us")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
#Remove Empty Row
#myCorpus <- myCorpus[-as.numeric(empty.rows)]
```{r}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
#inspect(tdm)
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
library(tm)
library(topicmodels)
library(gridExtra)
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 115)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
y<-ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
grid.arrange(y, y, ncol=2)
# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
library(wordcloud)
par(mar=c(.5,.5,.5,.5))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq <- subset(wordFreq, wordFreq >= 5)
wordFreq <- subset(wordFreq, wordFreq <= 115)
df2 <- data.frame(term = names(wordFreq), freq = wordFreq)
# word cloud
set.seed(375) # to make it reproducible
y<-wordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)
grid.arrange(y, y, ncol=2)
#remove sparse terms
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)
m2 <- as.matrix(myTdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
# cut tree into 10 clusters
rect.hclust(fit, k=10)
#remove sparse terms
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)
m2 <- as.matrix(myTdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
y<-plot(fit, asp =1, cex=.75)
# cut tree into 10 clusters
rect.hclust(fit, k=10)
grid.arrange(y, y, ncol=2)
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)
m2 <- as.matrix(myTdm2)
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
y<-plot(fit, asp =1, cex=.75)
# cut tree into 10 clusters
rect.hclust(fit, k=10)
grid.arrange(y, y, ncol=2)
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
clinton2 <- clinton[1:1437,]
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 6))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
#SECOND TRY
topics <- data.frame(date=as.Date(clinton2$created), topic)
y<-qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
grid.arrange(y, y, ncol=2)
y<-qplot(date, ..sum.., data=topics, geom="density",fill=term[topic], position="stack")
grid.arrange(y, y, ncol=2)
topics <- data.frame(date=as.Date(clinton2$created), topic)
y<-qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
grid.arrange(y, y, ncol=2)
```{r eval=FALSE, warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
write.csv(tweets.hc, file = "clinton")
## retrieve tweets from Twitter
library(twitteR)
Clinton <-userTimeline('HillaryClinton' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
# convert tweets to a data frame
tweets.hc <- twListToDF(Clinton)
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.hc$text[i], 60))
}
#shortcut for saving tweets to local maching
write.csv(tweets.hc, file = "clinton")
clinton <- read.csv("~/_data/TwitterAnalysis/clinton")
```{r eval=FALSE, warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
```{r eval=FALSE}
## retrieve tweets from Twitter
library(twitteR)
Clinton <-userTimeline('HillaryClinton' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
# convert tweets to a data frame
tweets.hc <- twListToDF(Clinton)
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.hc$text[i], 60))
}
#shortcut for saving tweets to local maching
write.csv(tweets.hc, file = "clinton")
```
library(twitteR)
Clinton <-userTimeline('HillaryClinton' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
# convert tweets to a data frame
tweets.hc <- twListToDF(Clinton)
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.hc$text[i], 60))
}
#shortcut for saving tweets to local maching
write.csv(tweets.hc, file = "clinton")
```
write.csv(tweets.hc, file = "clinton")
#clean up wrap text feature
for (i in c(1:2, 1500)) {
cat(paste0("[", i, "] "))
writeLines(strwrap(tweets.hc$text[i], 60))
}
write.csv(tweets.hc, file = "clinton")
Build a corpus and clean up
```{r}
#load the file saved locally
clinton <- read.csv("~/_data/TwitterAnalysis/clinton")
library(tm)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(clinton$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
#remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
#remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
#need to explain the concept of stop words.....
# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "will", "hillari", "rt", "us")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
library(tm)
library(topicmodels)
library(gridExtra)
#(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 115))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 115)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
y<-ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
grid.arrange(y, y, ncol=2)
# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
library(tm)
library(NLP)
#CLINTON
trump <- read.csv("~/_data/TwitterAnalysis/trump")#load the file saved locally
myCorpus <- Corpus(VectorSource(trump$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 115)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
grid.arrange(y, x, ncol=2)
# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
ibrary(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 115)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 75)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))
```{r warnings=FALSE}
# plot word cloud
library(wordcloud)
par(mar=c(.5,.5,.5,.5))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq <- subset(wordFreq, wordFreq >= 5)
wordFreq <- subset(wordFreq, wordFreq <= 75)
df2 <- data.frame(term = names(wordFreq), freq = wordFreq)
# word cloud
set.seed(375) # to make it reproducible
wordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)
```
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=10)# cut tree into 10 clusters
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
trump2 <- trump[1:1437,]
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 6))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topics <- data.frame(date=as.Date(clinton2$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
trump2 <- trump[1:1437,]
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 6))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topics <- data.frame(date=as.Date(trump2$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
