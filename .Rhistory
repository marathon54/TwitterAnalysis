library(twitteR)
tweets <- userTimeline("realDonaldTrump", n = 3200)
dim(tweets)
tweets
str(tweets)
View(tweets)
Inspect(Tweets)
searchTwitter("#beer", n=100)
Rtweets(n=37)
searchTwitter('charlie sheen', since='2011-03-01', until='2011-03-02')
libya<-searchTwitter("Muslim Brotherhood ", geocode='26.3339896,17.26921,20mi', n = 1500)
libya<-searchTwitter("@", geocode='26.3339896,17.26921,20mi', n = 1500)
libya<-searchTwitter("@", geocode='26.3339896,17.26921,200mi', n = 1500)
libya<-searchTwitter("@", geocode='26.3339896,17.26921,500mi', n = 1500)
libya<- twListToDF(libya2)
libya<- twListToDF(libya)
libya
view(libya)
View(libya)
libya2<-searchTwitter(" + Ø·Ø±Ø§Ø¨Ù„Ø³", geocode='26.3159237,20.5798821, 100mi', n =2500)
Inspect(libya)
View(libya)
x<-<U+0645><U+062D><U+0644><U+064A><U+0627> <U+0627><U+FEF7><U+0647><U+0644><U+064A> <U+0627><U+0644><U+0645><U+0635><U+0631><U+064A> <U+0648> <U+0639><U+0627><U+0644><U+0645><U+064A><U+0627> <U+0645><U+0643><U+0627><U+0628><U+064A> <U+062A><U+0644> <U+0623><U+0628><U+064A><U+0628> <U+0627><U+0644><U+0635><U+0647><U+064A><U+0648><U+0646><U+064A>
Ø·Ø±Ø§Ø¨Ù„Ø³,
x<-"Ø·Ø±Ø§Ø¨Ù„Ø³,"
x
dim(x)
str(x)
iconv(x, from = "UTF-8", to = "latin1", sub = NA, mark = TRUE, toRaw = FALSE)
x
Ø·Ø±Ø§Ø¨Ù„Ø³
x
install.packages('XML')
require(XML)
mydata.vectors <- character(0)
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
length(mydata.vectors)
write.csv(libya2, file = "LIBYA2")#shortcut for saving tweets to local maching
libya2<- twListToDF(libya2)# convert tweets to a data frame
write.csv(libya2, file = "LIBYA2")#shortcut for saving tweets to local maching
library(tm)
library(NLP)
libya <- read.csv("~/_data/TwitterAnalysis/LIBYA2")#load the file saved locally
myCorpus <- Corpus(VectorSource(libya$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 115))
encoding(libya2)
Encoding(libya2)
Sys.setlocale(locale="US")
Sys.setlocale(locale="Lybia")
Sys.setlocale(locale="Czech")
Sys.setlocale(locale="US")
View(libya)
View(libya)
tweets <- userTimeline("ShababLibya", n = 3200)
tweets
libya3<-searchTwitter("ShababLibya", geocode='26.3159237,20.5798821, 100mi', n =2500)
libya3<-searchTwitter("ShababLibya", n =2500)
libya2<-searchTwitter("#libya", geocode='26.3159237,20.5798821, 100mi', n =2500)
libya
head(libya)
help(searchTwitter)
libya2<-searchTwitter("#libya", lang = "en", geocode='26.3159237,20.5798821, 100mi', n =2500)
library(twitteR) ## retrieve tweets from Twitter
libya2<- twListToDF(libya2)# convert tweets to a data frame
write.csv(libya2, file = "LIBYA2")#shortcut for saving tweets to local maching
library(tm)
library(NLP)
#CLINTON
libya <- read.csv("~/_data/TwitterAnalysis/LIBYA2")#load the file saved locally
myCorpus <- Corpus(VectorSource(libya$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 115))
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 500))
(freq.terms <- findFreqTerms(tdm, lowfreq = 100, highfreq = 500))
(freq.terms <- findFreqTerms(tdm, lowfreq = 75, highfreq = 500))
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 500))
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 400))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 115)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
y<-ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
grid.arrange(y, x, ncol=2)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 400))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
grid.arrange(y, x, ncol=2)
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
(findAssocs(tdm, "tax", 0.2))
(findAssocs(tdm, "isi", 0.2))
# plot word cloud
library(wordcloud)
par(mar=c(.5,.5,.5,.5))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq <- subset(wordFreq, wordFreq >= 5)
wordFreq <- subset(wordFreq, wordFreq <= 115)
df2 <- data.frame(term = names(wordFreq), freq = wordFreq)
# word cloud
set.seed(375) # to make it reproducible
wordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=10)# cut tree into 10 clusters
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=10)# cut tree into 10 clusters
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
lda <- LDA(dtm.new, k = 5)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 20))
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 20))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topics <- data.frame(date=as.Date(clinton2$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
topics <- data.frame(date=as.Date(libya$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
qplot(date, ..count.., data=topics, geom="bar",fill=term[topic], position="stack")
ggplot(topics, aes(x = date, y=..count..)) + geom_point() + facet_grid(~ topic)
ggplot(topics, aes(x = date, y=..count..)) + geom_bar() + facet_grid(~ topic)
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=5)# cut tree into XX clusters
myStopwords
help(myStopwords)
myStopwords <- c(stopwords('english'), "available", "via", "libya")
myStopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```
Stem Completion step
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 400))
myStopwords <- c(stopwords('english'), "available", "via", "libya", "libyan")
myStopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 400))
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
myStopwords <- c(stopwords('english'), "available", "via", "amp", libya", "libyan") #add stopwords
myStopwords <- c(stopwords('english'), "available", "via", "amp", "libya", "libyan") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
myStopwords <- c(stopwords('english'), "available", "via", "libya", "libyan", "amp") #add stopwords
yCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
syria<-searchTwitter("#", lang = "en", geocode='36.2301786,37.2280928, 200mi', n =2500)
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
syria<-searchTwitter("#", lang = "en", geocode='36.2301786,37.2280928, 200mi', n =2500)
syria
syria<-searchTwitter("#syria", lang = "en", geocode='36.2301786,37.2280928, 200mi', n =3500)
syria<-searchTwitter(lang = "en", geocode='36.2301786,37.2280928, 200mi', n =3500)
syria<-searchTwitter(" ", lang = "en", geocode='36.2301786,37.2280928, 200mi', n =3500)
getTrends("current")
closestTrendLocations(36.2301786,37.2280928, 200mi)
closestTrendLocations(36.2301786,37.2280928)
closestTrendLocations(36.2301786,37.2280928)
getTrends(2343999)
getTrends(2343999, n=50)
closestTrendLocations(36.2301786,37.2280928)
getTrends(12766272)
closestTrendLocations(29.7071705,19.7196656)
closestTrendLocations(27.1089796,16.6019409)
closestTrendLocations(22.6535307,8.9645647)
getTrends(23424740)
x<-getTrends(23424740)
dim(x)
x
woeid = availableTrendLocations[1, "woeid"]
t1 <- getTrends(woeid)
woeid = availableTrendLocations[1, "woeid"]
CAR<-searchTwitter(" ", lang = "en", geocode='9.8914262,11.642202, 1000mi', n =3500)
CAR<- twListToDF(CAR)# convert tweets to a data frame
write.csv(libya2, file = "CAR")#shortcut for saving tweets to local maching
libya <- read.csv("~/_data/TwitterAnalysis/CAR")#load the file saved locally
CAR <- read.csv("~/_data/TwitterAnalysis/CAR")#load the file saved locally
myCorpus <- Corpus(VectorSource(CAR$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myStopwords <- c(stopwords('english'), "available", "via", "amp") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```{r warning=FALSE}
library(tm)
library(NLP)
AR <- read.csv("~/_data/TwitterAnalysis/CAR")#load the file saved locally
myCorpus <- Corpus(VectorSource(CAR$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myStopwords <- c(stopwords('english'), "available", "via", "amp") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
CAR<- twListToDF(CAR)# convert tweets to a data frame
CAR
write.csv(CAR, file = "CAR")#shortcut for saving tweets to local maching
CAR <- read.csv("~/_data/TwitterAnalysis/CAR")#load the file saved locally
myCorpus <- Corpus(VectorSource(CAR$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myStopwords <- c(stopwords('english'), "available", "via", "amp") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```
Frequent Words and Associations
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 50)
term.freq <- subset(term.freq, term.freq <= 400)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])
library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()
CAR <- read.csv("~/_data/TwitterAnalysis/CAR")#load the file saved locally
write.csv(CAR, file = "CAR1")#shortcut for saving tweets to local maching
library(tm)
library(NLP)
CAR2 <- read.csv("~/_data/TwitterAnalysis/CAR1")#load the file saved locally
myCorpus <- Corpus(VectorSource(CAR2$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myStopwords <- c(stopwords('english'), "available", "via", "amp") #add stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))
CAR<-searchTwitter(" ", lang = "en", geocode='9.8914262,11.642202, 1000mi', n =3500)
CAR
tail(CAR)
CAR<-searchTwitter("#", lang = "en", geocode='9.8914262,11.642202, 1000mi', n =3500)
tail(CAR)
Inspect(CAR)
library(twitteR) ## retrieve tweets from Twitter
CAR<- twListToDF(CAR)# convert tweets to a data frame
View(CAR)
View(CAR)
View(CAR)
CAR2
CAR2 <- read.csv("~/_data/TwitterAnalysis/CAR1")#load the file saved locally
CAR<-searchTwitter("#", lang = "en", geocode='9.8914262,11.642202, 1000mi', n =3500)
CAR2<- twListToDF(CAR)# convert tweets to a data frame
View(CAR2)
CAR<-searchTwitter("#", lang = "en", geocode='9.8914262,11.642202, 500mi', n =3500)
View(CAR)
CAR2<- twListToDF(CAR)# convert tweets to a data frame
View(CAR2)
CAR<-searchTwitter(" ", geocode='7.5725464,33.8020709, 500mi', n =35)
CAR<-searchTwitter(" ", geocode='7.5725464,33.8020709, 1000mi', n =35)
CAR<-searchTwitter("#", geocode='7.5725464,33.8020709, 1000mi', n =35)
CAR<-searchTwitter("#", geocode='7.5725464,33.8020709, 2000mi', n =35)
CAR<-searchTwitter("#", geocode='8.9631768,38.7781448, 2000mi', n =35)
CAR<-searchTwitter(" ", geocode='8.9631768,38.7781448, 2000mi', n =35)
syria<-searchTwitter("#syria", lang = "en", geocode='36.2301786,37.2280928, 200mi', n =3500)
tweets<-searchTwitter("#sudan ", geocode='15.516763, 32.141242, 1000mi', n =35)
tweets<-searchTwitter("#sudan", geocode='15.516763, 32.141242, 5000mi', n =35)
syria<-searchTwitter("#syria", lang = "en", geocode='36.2301786,37.2280928, 100mi', n =3500)
tweets<-searchTwitter("#syria", lang = "en", geocode='36.2301786,37.2280928, 100mi', n =3500)
tweets2<- twListToDF(tweets)# convert tweets to a data frame
write.csv(tweets2, file = "tweets2")#shortcut for saving tweets to local maching
View(tweets2)
HoCo<-searchTwitter("#", geocode='26.3339896,17.26921,20mi', n = 1500)
HoCo<-searchTwitter("", geocode='26.3339896,17.26921,200mi', n = 1500)
HoCo<-searchTwitter("", geocode='26.3339896,17.26921,300mi', n = 1500)
tweets2<- twListToDF(HoCo)
View(tweets2)
HoCo<-searchTwitter("", geocode='39.1626085,-76.9009079,100mi', n = 1500)
tweets2<- twListToDF(HoCo)
View(tweets2)
HoCo<-searchTwitter("", geocode='8.908132, 39.337193,100mi', n = 1500)
HoCo<-searchTwitter("", geocode='8.908132, 39.337193,1000mi', n = 1500)
HoCo<-searchTwitter("", geocode='8.908132, 39.337193,10000mi', n = 1500)
HoCo<-searchTwitter("", geocode='8.908132, 39.337193,100000mi', n = 1500)
tweets<-searchTwitter("#syria", lang = "en", geocode='36.2301786,37.2280928, 100mi', n =3500)
tweets2<- twListToDF(tweets)
View(tweets2)
dim(tweets2)
tweets<-searchTwitter("#syria", geocode='36.2301786,37.2280928, 100mi', n =3500)
tweets2<- twListToDF(tweets)
View(tweets2)
