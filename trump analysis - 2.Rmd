---
title: "Data Mining in Twitter - Proof of Concept for AFRICOM"
author: "Kevin A. Ryan (JHUAPL)"
date: "Monday, September 28, 2015"
output:
  word_document: default
  html_document:
    highlight: kate
    theme: cerulean
---

Setup code to enable access to Twitter API.  Explain the process here.  WHat is involved.

```{r eval=FALSE, warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
        consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
        consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
        access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
        access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```
----
```{r eval=FALSE}
library(twitteR) ## retrieve tweets from Twitter

#TRUMP
Trump <-userTimeline('realdonaldTrump' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)
tweets.dt <- twListToDF(Trump)# convert tweets to a data frame
        for (i in c(1:2, 1500)) { #clean up wrap text feature
                cat(paste0("[", i, "] "))
                writeLines(strwrap(tweets.dt$text[i], 60))}
write.csv(tweets.dt, file = "trump")#shortcut for saving tweets to local maching
```

Build a corpus and clean up 
```{r warning=FALSE}
library(tm)
library(NLP)

#CLINTON
trump <- read.csv("~/_data/TwitterAnalysis/trump")#load the file saved locally
myCorpus <- Corpus(VectorSource(trump$text))# build a corpus, and specify the source to be character vectors
myCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)# remove URLs
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)# leave only English letters or spaces
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace
myCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion
myCorpus <- tm_map(myCorpus, stemDocument)# stem words
```

Stem Completion step
```{r,warning=FALSE}
library(tm)
myCorpus <- Corpus(VectorSource(myCorpus))
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
```

Frequent Words and Associations
```{r warning=FALSE, comment=FALSE}
library(tm)
library(topicmodels)
library(gridExtra)
(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 35)
term.freq <- subset(term.freq, term.freq <= 75)
df <- data.frame(term = names(term.freq), freq = term.freq)
df$term <-factor(df$term, levels=df[order(df$freq), "term"])

library(ggplot2)
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +
xlab("Terms") + ylab("Count") + coord_flip()

# which words are associated with the term "tax" ?
(findAssocs(tdm, "tax", 0.2))
```

WOrd cloud mutherfucker!
```{r warnings=FALSE}
# plot word cloud
library(wordcloud)
par(mar=c(.5,.5,.5,.5))
 m <- as.matrix(tdm)
# calculate the frequency of words and sort it descendingly by frequency
wordFreq <- sort(rowSums(m), decreasing=TRUE)
wordFreq <- subset(wordFreq, wordFreq >= 5)
wordFreq <- subset(wordFreq, wordFreq <= 75)
df2 <- data.frame(term = names(wordFreq), freq = wordFreq)

# word cloud
set.seed(375) # to make it reproducible

wordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)
```

clustering....In the above dendrogram, we can see the topics in the tweets. Words \analysis", \network" and \social" are clustered into one group, because there are a couple of tweets on social network analysis. The second cluster from left comprises \positions", \postdoctoral" and \research", and they are clustered into one group because of tweets on vacancies of research and postdoctoral positions. We can also see cluster on time series, R packages, parallel computing, R codes and examples, and tutorial and slides. The rightmost three clusters consists of \r", \data"and\mining", which are the keywords of @RDataMining tweets.
```{r}
library(gridExtra)
myTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms
m2 <- as.matrix(myTdm2)
distMatrix <- dist(scale(m2))# cluster terms
fit <- hclust(distMatrix, method="ward.D")
par(mar=c(2,2,2,2))
plot(fit, asp =1, cex=.75)
rect.hclust(fit, k=10)# cut tree into 10 clusters
```

topic modeling2
```{r}
library(tm)
library(topicmodels)
library(gridExtra)
dtm <- as.DocumentTermMatrix(tdm) #1437 documents
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words
trump2 <- trump[1:1437,] 
lda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)
(term <- terms(lda, 6))
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
topic <- topics(lda, 1)
topics <- data.frame(date=as.Date(trump2$created), topic)
qplot(date, ..count.., data=topics, geom="density",fill=term[topic], position="stack")
```


