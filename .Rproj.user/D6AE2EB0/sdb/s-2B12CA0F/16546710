{
    "contents" : "---\ntitle: \"Analysis of Twitter Data in Libya\"\nauthor: \"Kevin A. Ryan (JHUAPL)\"\ndate: \"Friday, September 25, 2015\"\noutput:\n  html_document:\n    highlight: kate\n    theme: spacelab\n---\n\n\nStep 1: Ensure proper packages are installed.\n```{r eval=FALSE}\ninstall.packages(c(\"devtools\", \"rjson\", \"bit64\"))\n\n```\n\nStep 2: Retrieving Text from Twitter\n```{r eval=False, cache=TRUE,warning=FALSE}\nlibrary(devtools)\nlibrary(httr)\nlibrary(twitteR)\n\ndownload.file(url=\"http://curl.haxx.se/ca/cacert.pem\", destfile=\"cacert.pem\")\n\nconsumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'\nconsumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'\naccess_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'\naccess_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'\nsetup_twitter_oauth(consumer_key,\n                    consumer_secret,\n                    access_token,\n                    access_secret)\n\n#Grab from the Twitter pipeline the last 100 tweets, worldwide, that were marked with the hashtag 'libya'\nlibya<-searchTwitter(\"#libya\", n=500)\n```\n\nExamples of other Twitter Queries\n```{r eval = FALSE }\n## search based on a hashtag:\nsearchTwitter(\"#beer\", n=100)\nRtweets(n=37)\n\n## Search between two dates\nsearchTwitter('charlie sheen', since='2011-03-01', until='2011-03-02')\n\n## geocoded results\npats<-searchTwitter('patriots', geocode='42.375,-71.1061111,100mi', n = 1000)\n\n## using resultType\nsearchTwitter('world cup+brazil', resultType=\"popular\", n=150)\nsearchTwitter('from:hadleywickham', resultType=\"recent\", n=10)\n```\n\nNext steps in the Libya Data processing\n```{r}\n# Convert the tweets into a data frame\ndf<-do.call(\"rbind\", lapply(libya, as.data.frame))\ndim(df)\nView(df)\n```\n\nStep 3:  Convert the data frame into a corpus\n```{r}\nlibrary(tm)\nmyCorpus<-Corpus(VectorSource(df$text))\n# convert to lower case\nmyCorpus <- tm_map(myCorpus, tolower)\n# remove punctuation\nmyCorpus <- tm_map(myCorpus, removePunctuation)\n# remove numbers\nmyCorpus <- tm_map(myCorpus, removeNumbers)\n# remove URLs\nremoveURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, removeURL)\n# add two extra stop words: \"available\" and \"via\"\nmyStopwords <- c(stopwords('english'), \"available\", \"via\")\n# remove \"r\" and \"big\" from stopwords\nmyStopwords <- setdiff(myStopwords, c(\"r\", \"big\"))\n# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)\n```\n\nStep 5: Stemming Words\nIn many applications, words need to be stemmed to retrieve their radicals, so that various forms derived from a stem would be taken as the same when counting word frequency. For instance, words \\update\",\\updated\" and \\updating\" would all be stemmed to \\updat\". Word stemming can be done with the snowball stemmer, which requires packages Snowball , RWeka, rJava and RWekajars. After that, we can complete the stems to their original forms, i.e., \\update\" for the above example, so that the words would look normal. This can be achieved with function stemCompletion().\n```{r}\n# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpusCopy <- myCorpus\n# stem words\nmyCorpus <- tm_map(myCorpus, stemDocument)\n# inspect documents (tweets) numbered 11 to 15\n# inspect(myCorpus[11:15])\n# The code below is used for to make text fit for paper width\nfor (i in 1:1500) {\ncat(paste(\"[[\", i, \"]] \", sep=\"\"))\nwriteLines(strwrap(myCorpus[[i]], width=73))\n}\n```\n\nStep 6\n```{r}myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))\n> myTdm\n\n```\n\n",
    "created" : 1443200133257.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "39|20|53|3|\n",
    "hash" : "3979075376",
    "id" : "16546710",
    "lastKnownWriteTime" : 1443212355,
    "path" : "~/_data/TwitterAnalysis/TwitterAnalysis.Rmd",
    "project_path" : "TwitterAnalysis.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}