{
    "contents" : "---\ntitle: \"Data Mining in Twitter - Proof of Concept for AFRICOM\"\nauthor: \"Kevin A. Ryan (JHUAPL)\"\ndate: \"Monday, September 28, 2015\"\noutput:\n  word_document: default\n  html_document:\n    highlight: kate\n    theme: cerulean\n---\n\nSetup code to enable access to Twitter API.  Explain the process here.  WHat is involved.\n\n\n```{r eval=FALSE, warning=FALSE}\nlibrary(devtools)\nlibrary(httr)\nlibrary(rjson)\nlibrary(bit64)\nlibrary(twitteR)\ndownload.file(url=\"http://curl.haxx.se/ca/cacert.pem\", destfile=\"cacert.pem\")\n        consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'\n        consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'\n        access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'\n        access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'\nsetup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)\n\n#tweets <- userTimeline(\"realDonaldTrump\", n = 3200)\nHoCo<-searchTwitter(\"\", geocode='8.908132, 39.337193,100mi', n = 1500)\n\n\ntweets<-searchTwitter(\"#syria\", geocode='36.2301786,37.2280928, 100mi', n =3500)\ntweets2<- twListToDF(tweets)\n#tweets<-searchTwitter(\"#sudan\", geocode='15.516763, 32.141242, 5000mi', n =35)\n\n#al shabab:  @HSMPress\n#Tripoli: 32.882108, 13.190482\n#libya: 26.3339896,17.26921\n#Mauritania: 19.630928, -10.040291\n#Ethiopa: 9.485258, 39.928106\n#columbia, md: 39.214928, -76.855298\n## Search between two dates\n        #searchTwitter('charlie sheen', since='2011-03-01', until='2011-03-02')\n## geocoded results\n        #searchTwitter('patriots', geocode='42.375,-71.1061111,10mi')\n## using resultType\n        #searchTwitter('world cup+brazil', resultType=\"popular\", n=15)\n        #searchTwitter('from:hadleywickham', resultType=\"recent\", n=10)\n        #searchTwitter(\"#beer\", n=100)\n```\n\n\n----\n```{r eval=FALSE}\nlibrary(twitteR) ## retrieve tweets from Twitter\ntweets2<- twListToDF(tweets)# convert tweets to a data frame\nwrite.csv(tweets2, file = \"tweets2\")#shortcut for saving tweets to local maching\n\n```\n\nBuild a corpus and clean up \n```{r warning=FALSE}\nlibrary(tm)\nlibrary(NLP)\n\nCAR2 <- read.csv(\"~/_data/TwitterAnalysis/CAR1\")#load the file saved locally\nmyCorpus <- Corpus(VectorSource(CAR2$text))# build a corpus, and specify the source to be character vectors\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)# remove URLs\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)# leave only English letters or spaces\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\nmyCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation\nmyCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers\nmyStopwords <- c(stopwords('english'), \"available\", \"via\", \"amp\") #add stopwords\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace\nmyCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpus <- tm_map(myCorpus, stemDocument)# stem words\n```\n\nStem Completion step\n```{r,warning=FALSE}\nlibrary(tm)\nmyCorpus <- Corpus(VectorSource(myCorpus))\ntdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))\n```\n\nFrequent Words and Associations\n```{r warning=FALSE, comment=FALSE}\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(gridExtra)\n(freq.terms <- findFreqTerms(tdm, lowfreq = 50, highfreq = 1000))\nterm.freq <- rowSums(as.matrix(tdm))\nterm.freq <- subset(term.freq, term.freq >= 50)\nterm.freq <- subset(term.freq, term.freq <= 400)\ndf <- data.frame(term = names(term.freq), freq = term.freq)\ndf$term <-factor(df$term, levels=df[order(df$freq), \"term\"])\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = term, y = freq)) + geom_bar(stat = \"identity\") +\nxlab(\"Terms\") + ylab(\"Count\") + coord_flip()\n\n# which words are associated with the term \"tax\" ?\n(findAssocs(tdm, \"isi\", 0.2))\n```\n\nWOrd cloud mutherfucker!\n```{r warnings=FALSE}\n# plot word cloud\nlibrary(wordcloud)\npar(mar=c(.5,.5,.5,.5))\n m <- as.matrix(tdm)\n# calculate the frequency of words and sort it descendingly by frequency\nwordFreq <- sort(rowSums(m), decreasing=TRUE)\nwordFreq <- subset(wordFreq, wordFreq >= 5)\nwordFreq <- subset(wordFreq, wordFreq <= 115)\ndf2 <- data.frame(term = names(wordFreq), freq = wordFreq)\n\n# word cloud\nset.seed(375) # to make it reproducible\n\nwordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)\n```\n\n\n```{r}\nlibrary(gridExtra)\nmyTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms\nm2 <- as.matrix(myTdm2)\ndistMatrix <- dist(scale(m2))# cluster terms\nfit <- hclust(distMatrix, method=\"ward.D\")\npar(mar=c(2,2,2,2))\nplot(fit, asp =1, cex=.75)\nrect.hclust(fit, k=5)# cut tree into XX clusters\n```\n\ntopic modeling2\n```{r}\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(gridExtra)\ndtm <- as.DocumentTermMatrix(tdm) #1437 documents\nrowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document\ndtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words\nclinton2 <- clinton[1:2500,] \nlda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)\n(term <- terms(lda, 20))\nterm <- apply(term, MARGIN = 2, paste, collapse = \", \")\ntopic <- topics(lda, 1)\ntopics <- data.frame(date=as.Date(libya$created), topic)\nqplot(date, ..count.., data=topics, geom=\"density\",fill=term[topic], position=\"stack\")\n\nqplot(date, ..count.., data=topics, geom=\"bar\",fill=term[topic], position=\"stack\")\n\nggplot(topics, aes(x = date, y=..count..)) + geom_point() + facet_grid(~ topic)\n\nggplot(topics, aes(x = date, y=..count..)) + geom_bar() + facet_grid(~ topic)\n```\n\n\n",
    "created" : 1444222494286.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "140|6|160|3|\n",
    "hash" : "689511160",
    "id" : "57189639",
    "lastKnownWriteTime" : 1444333704,
    "path" : "~/_data/TwitterAnalysis/africa.Rmd",
    "project_path" : "africa.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}