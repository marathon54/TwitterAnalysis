{
    "contents" : "---\ntitle: \"Analysis of Twitter Data in Libya\"\nauthor: \"Kevin A. Ryan (JHUAPL)\"\ndate: \"Friday, September 25, 2015\"\noutput:\n  html_document:\n    highlight: kate\n    theme: spacelab\n---\n\n\nStep 1: Ensure proper packages are installed.\n```{r eval=FALSE}\ninstall.packages(c(\"bit64\", \"rjson\")) #\"devtools\", \"rjson\",\n\n```\n\nStep 2: Pulling Data from Twitter\n```{r eval=FALSE,warning=FALSE}\nlibrary(devtools)\nlibrary(httr)\nlibrary(rjson)\nlibrary(bit64)\nlibrary(twitteR)\n\ndownload.file(url=\"http://curl.haxx.se/ca/cacert.pem\", destfile=\"cacert.pem\")\n\nconsumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'\nconsumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'\naccess_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'\naccess_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'\nsetup_twitter_oauth(consumer_key,\n                    consumer_secret,\n                    access_token,\n                    access_secret)\n\nlibrary(twitteR)\ntweets <- userTimeline(\"realDonaldTrump\", n = 3200)\n#HoCo<-searchTwitter(\"#\", geocode='39.214928,-76.855298,20mi', n = 1500)\n\n#al shabab:  @HSMPress\n#Tripoli: 32.882108, 13.190482\n#libya: 26.3339896,17.26921\n#Mauritania: 19.630928, -10.040291\n#Ethiopa: 9.485258, 39.928106\n#columbia, md: 39.214928, -76.855298\n```\n\nExamples of other Twitter Queries\n```{r eval = FALSE }\n\n# Convert the tweets into a data frame\ndf<-do.call(\"rbind\", lapply(tweets, as.data.frame))\ndim(df)\nView(df)\n\n#Rtweets(n=1000, lang=NULL, since=NULL, ...)\n\n\n## search based on a hashtag (for reference):\n\n#searchTwitter(searchString, n=25, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL, resultType=NULL, retryOnRateLimit=120, ...) Rtweets(n=25, lang=NULL, since=NULL, ...)\n\n#Lookup table for [ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)\n\n## using resultType\n        #searchTwitter('world cup+brazil', resultType=\"popular\", n=150)\n        #searchTwitter('from:hadleywickham', resultType=\"recent\", n=10)\n```\n\nStep 3:  Convert the data frame into a corpus and transform text\n```{r}\nlibrary(tm)\nlibrary(stringr)\nrequire(tm)\nrequire(tm.plugin.webmining)\nrequire(SnowballC)\n\n# cleanup:\n#usableText=str_replace_all(df$text,\"[^[:graph:]]\", \" \") \n\n# build a corpus, and specify the source to be character vectors\nmyCorpus <- Corpus(VectorSource(df$text))\n\n# convert to lower case\nmyCorpus <- tm_map(myCorpus, tolower)\n\n# remove punctuation\nmyCorpus <- tm_map(myCorpus, removePunctuation)\n\n# remove numbers\nmyCorpus <- tm_map(myCorpus, removeNumbers)\n\n# remove URLs\nremoveURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, removeURL)\n\n# add two extra stop words: \"available\" and \"via\"\nmyStopwords <- c(stopwords('english'), \"available\", \"via\")\n\n# remove \"r\" and \"big\" from stopwords\nmyStopwords <- setdiff(myStopwords, \"r\", \"big\")\n\n# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)\n\n```\n\nStep 5: Stemming Words\nIn many applications, words need to be stemmed to retrieve their radicals, so that various forms derived from a stem would be taken as the same when counting word frequency. For instance, words \\update\",\\updated\" and \\updating\" would all be stemmed to \\updat\". Word stemming can be done with the snowball stemmer, which requires packages Snowball , RWeka, rJava and RWekajars. After that, we can complete the stems to their original forms, i.e., \\update\" for the above example, so that the words would look normal. This can be achieved with function stemCompletion().\n```{r}\n# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpusCopy <- myCorpus\n# stem words\nmyCorpus <- tm_map(myCorpus, stemDocument)\n# inspect documents (tweets) numbered 11 to 15\n# inspect(myCorpus[11:15])\n# The code below is used for to make text fit for paper width\n\n#stem completion\nmyCorpus <- tm_map(myCorpus, stemDocument)\ninspect(myCorpus[1:50])\n```\n\nStep 6: Build a term-document matrix \nA term-document matrix represents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document. Alternatively, one can also build a document-term matrix by swapping row and column. In this section, we build a term-document matrix from the above processed corpus with function TermDocumentMatrix(). With its default setting, terms with less than three characters are discarded. To keep \\r\" in the matrix, we set the range of wordLengths in the example below.\n\n```{r}\nmyTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))\nmyTdm\nmyTdm <- TermDocumentMatrix(myCorpus, control=list(minWordLength=1))\n```\n\nStep 7: Frequent Terms and Associations\n\n```{r}\nfindFreqTerms(myTdm, lowfreq=50)\n\ntermFrequency <- rowSums(as.matrix(myTdm))\ntermFrequency <- subset(termFrequency, termFrequency>=50)\npar(mar=c(1.5,1.5,1.5,1.5))\nlibrary(ggplot2)\nc<-qplot(names(termFrequency), termFrequency, xlab=\"Terms\") \nc + coord_flip()\n\nplot(df)\n\n\n```\n\n\n",
    "created" : 1444222417634.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3041221943",
    "id" : "766FD48D",
    "lastKnownWriteTime" : 1443485927,
    "path" : "~/_data/TwitterAnalysis/TwitterAnalysis.Rmd",
    "project_path" : "TwitterAnalysis.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}