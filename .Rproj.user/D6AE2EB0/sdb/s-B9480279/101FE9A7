{
    "contents" : "---\ntitle: \"Data Mining in Twitter - Proof of Concept for AFRICOM\"\nauthor: \"Kevin A. Ryan (JHUAPL)\"\ndate: \"Monday, September 28, 2015\"\noutput:\n  word_document: default\n  html_document:\n    highlight: kate\n    theme: cerulean\n---\n\nSetup code to enable access to Twitter API.  Explain the process here.  WHat is involved.\n\n```{r eval=FALSE, warning=FALSE}\nlibrary(devtools)\nlibrary(httr)\nlibrary(rjson)\nlibrary(bit64)\nlibrary(twitteR)\ndownload.file(url=\"http://curl.haxx.se/ca/cacert.pem\", destfile=\"cacert.pem\")\n        consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'\n        consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'\n        access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'\n        access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'\nsetup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)\n```\n----\n```{r eval=FALSE}\nlibrary(twitteR) ## retrieve tweets from Twitter\n\n#TRUMP\nTrump <-userTimeline('realdonaldTrump' , n=1500, maxID=NULL, sinceID=NULL, includeRts=TRUE,excludeReplies=TRUE)\ntweets.dt <- twListToDF(Trump)# convert tweets to a data frame\n        for (i in c(1:2, 1500)) { #clean up wrap text feature\n                cat(paste0(\"[\", i, \"] \"))\n                writeLines(strwrap(tweets.dt$text[i], 60))}\nwrite.csv(tweets.dt, file = \"trump\")#shortcut for saving tweets to local maching\n```\n\nBuild a corpus and clean up \n```{r warning=FALSE}\nlibrary(tm)\nlibrary(NLP)\n\n#CLINTON\ntrump <- read.csv(\"~/_data/TwitterAnalysis/trump\")#load the file saved locally\nmyCorpus <- Corpus(VectorSource(trump$text))# build a corpus, and specify the source to be character vectors\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower))# convert to lower case\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)# remove URLs\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)# leave only English letters or spaces\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\nmyCorpus <- tm_map(myCorpus, removePunctuation)#remove punctuation\nmyCorpus <- tm_map(myCorpus, removeNumbers)#remove numbers\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)# remove stopwords from corpus\nmyCorpus <- tm_map(myCorpus, stripWhitespace)# remove extra whitespace\nmyCorpusCopy <- myCorpus# keep a copy of corpus to use later as a dictionary for stem completion\nmyCorpus <- tm_map(myCorpus, stemDocument)# stem words\n```\n\nStem Completion step\n```{r,warning=FALSE}\nlibrary(tm)\nmyCorpus <- Corpus(VectorSource(myCorpus))\ntdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))\n```\n\nFrequent Words and Associations\n```{r warning=FALSE, comment=FALSE}\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(gridExtra)\n(freq.terms <- findFreqTerms(tdm, lowfreq = 35, highfreq = 75))\nterm.freq <- rowSums(as.matrix(tdm))\nterm.freq <- subset(term.freq, term.freq >= 35)\nterm.freq <- subset(term.freq, term.freq <= 75)\ndf <- data.frame(term = names(term.freq), freq = term.freq)\ndf$term <-factor(df$term, levels=df[order(df$freq), \"term\"])\n\nlibrary(ggplot2)\nggplot(df, aes(x = term, y = freq)) + geom_bar(stat = \"identity\") +\nxlab(\"Terms\") + ylab(\"Count\") + coord_flip()\n\n# which words are associated with the term \"tax\" ?\n(findAssocs(tdm, \"tax\", 0.2))\n```\n\nWOrd cloud mutherfucker!\n```{r warnings=FALSE}\n# plot word cloud\nlibrary(wordcloud)\npar(mar=c(.5,.5,.5,.5))\n m <- as.matrix(tdm)\n# calculate the frequency of words and sort it descendingly by frequency\nwordFreq <- sort(rowSums(m), decreasing=TRUE)\nwordFreq <- subset(wordFreq, wordFreq >= 5)\nwordFreq <- subset(wordFreq, wordFreq <= 75)\ndf2 <- data.frame(term = names(wordFreq), freq = wordFreq)\n\n# word cloud\nset.seed(375) # to make it reproducible\n\nwordcloud(words=df2$term, freq=df2$freq, scale = c(1.5 ,.1),min.freq=5, random.order=T, fixed.asp=F, rot.per= 0, use.r.layout =T)\n```\n\nclustering....In the above dendrogram, we can see the topics in the tweets. Words \\analysis\", \\network\" and \\social\" are clustered into one group, because there are a couple of tweets on social network analysis. The second cluster from left comprises \\positions\", \\postdoctoral\" and \\research\", and they are clustered into one group because of tweets on vacancies of research and postdoctoral positions. We can also see cluster on time series, R packages, parallel computing, R codes and examples, and tutorial and slides. The rightmost three clusters consists of \\r\", \\data\"and\\mining\", which are the keywords of @RDataMining tweets.\n```{r}\nlibrary(gridExtra)\nmyTdm2 <- removeSparseTerms(tdm, sparse=0.97)#remove sparse terms\nm2 <- as.matrix(myTdm2)\ndistMatrix <- dist(scale(m2))# cluster terms\nfit <- hclust(distMatrix, method=\"ward.D\")\npar(mar=c(2,2,2,2))\nplot(fit, asp =1, cex=.75)\nrect.hclust(fit, k=10)# cut tree into 10 clusters\n```\n\ntopic modeling2\n```{r}\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(gridExtra)\ndtm <- as.DocumentTermMatrix(tdm) #1437 documents\nrowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document\ndtm.new   <- dtm[rowTotals> 0, ]   #remove all docs without words\ntrump2 <- trump[1:1437,] \nlda <- LDA(dtm.new, k = 8)#Latent Dirichlet allocation (LDA)\n(term <- terms(lda, 6))\nterm <- apply(term, MARGIN = 2, paste, collapse = \", \")\ntopic <- topics(lda, 1)\ntopics <- data.frame(date=as.Date(trump2$created), topic)\nqplot(date, ..count.., data=topics, geom=\"density\",fill=term[topic], position=\"stack\")\n```\n\n\n",
    "created" : 1443487589391.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1781539010",
    "id" : "101FE9A7",
    "lastKnownWriteTime" : 1443642897,
    "path" : "~/_data/TwitterAnalysis/trump analysis - 2.Rmd",
    "project_path" : "trump analysis - 2.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}