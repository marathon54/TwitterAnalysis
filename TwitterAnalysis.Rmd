---
title: "Analysis of Twitter Data in Libya"
author: "Kevin A. Ryan (JHUAPL)"
date: "Friday, September 25, 2015"
output:
  html_document:
    highlight: kate
    theme: spacelab
---


Step 1: Ensure proper packages are installed.
```{r eval=FALSE}
install.packages(c("bit64", "rjson")) #"devtools", "rjson",

```

Step 2: Pulling Data from Twitter
```{r eval=FALSE,warning=FALSE}
library(devtools)
library(httr)
library(rjson)
library(bit64)
library(twitteR)

download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")

consumer_key <- 'cD5uXJMuyJfLkodDvYYchEUuM'
consumer_secret <- 'FLVzv3HhHnsSr0qRfnq7ZmCdCWrQS24NUbnREUvHNFhEc89bVg'
access_token <- '740674741-PPuLbrOHGQCYqrh9C9E9AG8I57st4Akj1NBmB2V5'
access_secret <- 'oxsCu5UE9Q6QVowgd98Ds7qFeJIesI9Z3zGk5qDdP3S4m'
setup_twitter_oauth(consumer_key,
                    consumer_secret,
                    access_token,
                    access_secret)

library(twitteR)
tweets <- userTimeline("realDonaldTrump", n = 3200)
#HoCo<-searchTwitter("#", geocode='39.214928,-76.855298,20mi', n = 1500)

#al shabab:  @HSMPress
#Tripoli: 32.882108, 13.190482
#libya: 26.3339896,17.26921
#Mauritania: 19.630928, -10.040291
#Ethiopa: 9.485258, 39.928106
#columbia, md: 39.214928, -76.855298
```

Examples of other Twitter Queries
```{r eval = FALSE }

# Convert the tweets into a data frame
df<-do.call("rbind", lapply(tweets, as.data.frame))
dim(df)
View(df)

#Rtweets(n=1000, lang=NULL, since=NULL, ...)


## search based on a hashtag (for reference):

#searchTwitter(searchString, n=25, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL, resultType=NULL, retryOnRateLimit=120, ...) Rtweets(n=25, lang=NULL, since=NULL, ...)

#Lookup table for [ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)

## using resultType
        #searchTwitter('world cup+brazil', resultType="popular", n=150)
        #searchTwitter('from:hadleywickham', resultType="recent", n=10)
```

Step 3:  Convert the data frame into a corpus and transform text
```{r}
library(tm)
library(stringr)
require(tm)
require(tm.plugin.webmining)
require(SnowballC)

# cleanup:
#usableText=str_replace_all(df$text,"[^[:graph:]]", " ") 

# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(df$text))

# convert to lower case
myCorpus <- tm_map(myCorpus, tolower)

# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)

# add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")

# remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, "r", "big")

# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

```

Step 5: Stemming Words
In many applications, words need to be stemmed to retrieve their radicals, so that various forms derived from a stem would be taken as the same when counting word frequency. For instance, words \update",\updated" and \updating" would all be stemmed to \updat". Word stemming can be done with the snowball stemmer, which requires packages Snowball , RWeka, rJava and RWekajars. After that, we can complete the stems to their original forms, i.e., \update" for the above example, so that the words would look normal. This can be achieved with function stemCompletion().
```{r}
# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy <- myCorpus
# stem words
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect documents (tweets) numbered 11 to 15
# inspect(myCorpus[11:15])
# The code below is used for to make text fit for paper width

#stem completion
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:50])
```

Step 6: Build a term-document matrix 
A term-document matrix represents the relationship between terms and documents, where each row stands for a term and each column for a document, and an entry is the number of occurrences of the term in the document. Alternatively, one can also build a document-term matrix by swapping row and column. In this section, we build a term-document matrix from the above processed corpus with function TermDocumentMatrix(). With its default setting, terms with less than three characters are discarded. To keep \r" in the matrix, we set the range of wordLengths in the example below.

```{r}
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
myTdm
myTdm <- TermDocumentMatrix(myCorpus, control=list(minWordLength=1))
```

Step 7: Frequent Terms and Associations

```{r}
findFreqTerms(myTdm, lowfreq=50)

termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=50)
par(mar=c(1.5,1.5,1.5,1.5))
library(ggplot2)
c<-qplot(names(termFrequency), termFrequency, xlab="Terms") 
c + coord_flip()

plot(df)


```


